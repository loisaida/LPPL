{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loisaida/LPPL/blob/master/Momentum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Yw1o-w3gGTX",
        "outputId": "833f0261-83c2-4d08-dd8a-eff38a7d93b8"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/single_hist_record.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0fe7691f3ad8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Step 1: Read the initial rows to get the start and end dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minitial_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/single_hist_record.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract start and end dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/single_hist_record.csv'"
          ]
        }
      ],
      "source": [
        "######################################################MOMENTUM FACTOR CALCULATION###########################\n",
        "#IN\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Step 1: Read the initial rows to get the start and end dates\n",
        "initial_rows = pd.read_csv('/content/single_hist_record.csv', nrows=2, header=None)\n",
        "\n",
        "# Extract start and end dates\n",
        "start_date = initial_rows.iloc[0, 1]\n",
        "end_date = initial_rows.iloc[1, 1]\n",
        "\n",
        "# Step 2: Read the main data, skipping the first three rows and setting 'ID' as the index\n",
        "data_df = pd.read_csv('/content/single_hist_record.csv', skiprows=3, index_col='ID')\n",
        "\n",
        "# Step 3: Read the reference file, setting 'ID' as the index\n",
        "ref_df = pd.read_csv('/content/ig_esp_data_ref_3_6.csv', index_col='ID')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################calculates winners and losers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "\n",
        "PERCENT_WINNER = 0.2  # Adjust between 0 and 1 as needed\n",
        "PERCENT_LOSER = 0.2  # Adjust between 0 and 1 as needed\n",
        "\n",
        "def flag_winners_losers(group_df, percent_winners, percent_losers):\n",
        "    # Filter to include only quintiles 1 and 5\n",
        "    quintiles_of_interest = group_df[group_df['DTS_Quintile'].isin([1, 5])]\n",
        "\n",
        "    # Sort by 6m_total_return descending\n",
        "    quintiles_of_interest = quintiles_of_interest.sort_values(by='6m_total_return', ascending=False)\n",
        "\n",
        "    # Determine the indices for winners and losers based on the specified percentages\n",
        "    num_entries = len(quintiles_of_interest)\n",
        "    winners_cutoff = int(num_entries * percent_winners)\n",
        "    losers_cutoff = int(num_entries * percent_losers)\n",
        "\n",
        "    # Initialize flags\n",
        "    quintiles_of_interest['IsWinner'] = 0\n",
        "    quintiles_of_interest['IsLoser'] = 0\n",
        "\n",
        "    # Apply flags\n",
        "    quintiles_of_interest.iloc[:winners_cutoff, quintiles_of_interest.columns.get_loc('IsWinner')] = 1\n",
        "    quintiles_of_interest.iloc[-losers_cutoff:, quintiles_of_interest.columns.get_loc('IsLoser')] = 1\n",
        "\n",
        "    # Merge flags back to the main DataFrame based on ID\n",
        "    group_df = group_df.merge(quintiles_of_interest[['ID', 'IsWinner', 'IsLoser']], on='ID', how='left').fillna({'IsWinner': 0, 'IsLoser': 0})\n",
        "\n",
        "    return group_df\n",
        "\n",
        "def process_multi_structure_data_to_db(filepath, db_path):\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        # Your existing code for reading the data and iterating over date columns...\n",
        "\n",
        "            if not group_df.empty:\n",
        "                group_df['DTS'] = group_df['DURATION'] * group_df['SPREAD']\n",
        "                group_df['DTS_Quintile'] = pd.qcut(group_df['DTS'], 5, labels=False) + 1\n",
        "\n",
        "                # Assign DataFrameID\n",
        "                group_df['DataFrameID'] = dataframe_id\n",
        "\n",
        "                # Correctly flag winners and losers using the function\n",
        "                group_df = flag_winners_losers(group_df, PERCENT_WINNER, PERCENT_LOSER)\n",
        "\n",
        "                # Prepare DataFrame for insertion, ensuring column names match your SQLite table schema\n",
        "                columns_for_db = ['DataFrameID', 'ID', 'DURATION', 'SPREAD', '1m_total_return', '6m_total_return', 'DTS', 'DTS_Quintile', 'IsWinner', 'IsLoser']\n",
        "                group_df = group_df[columns_for_db]\n",
        "\n",
        "                # Insert updated DataFrameEntries into the database\n",
        "                group_df.to_sql('DataFrameEntries', conn, if_exists='append', index=False)\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "    return \"Data processed and saved to database.\"\n",
        "\n",
        "# Replace placeholders with actual paths\n",
        "process_multi_structure_data_to_db('/content/hy-historical-return-data-static.csv', '/content/esp_hy_data_analysis.db')\n"
      ],
      "metadata": {
        "id": "3pwm37JKLgnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RsO9-LDYgJjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c44971e6-2230-4930-ca3d-0a96ebd549dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "unable to open database file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09ca85b86f33>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Connect to the SQLite database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/database/esp_hy_data_analysis.db'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fetch DataFrameIDs along with their corresponding start dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
          ]
        }
      ],
      "source": [
        "############ calculates the mean of a mean of the 1_month_total_return for winners and losers for each month (DataFrame), and then displaying that in a table\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "# Connect to the SQLite database\n",
        "db_path = '/content/drive/MyDrive/database/esp_hy_data_analysis.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Fetch DataFrameIDs along with their corresponding start dates\n",
        "query_dates = \"\"\"\n",
        "SELECT DataFrameID, StartDate\n",
        "FROM DataFrames\n",
        "ORDER BY StartDate\n",
        "\"\"\"\n",
        "df_dates = pd.read_sql_query(query_dates, conn)\n",
        "\n",
        "# Initialize an empty DataFrame for the summary\n",
        "summary_df = pd.DataFrame(columns=['date', 'winner_mean', 'loser_mean'])\n",
        "\n",
        "for _, row in df_dates.iterrows():\n",
        "    df_id = row['DataFrameID']\n",
        "    start_date = row['StartDate']\n",
        "\n",
        "    # Query to retrieve 1m_total_return, IsWinner, and IsLoser for the current DataFrameID\n",
        "    query = f\"\"\"\n",
        "    SELECT 1m_total_return, IsWinner, IsLoser\n",
        "    FROM DataFrameEntries\n",
        "    WHERE DataFrameID = {df_id}\n",
        "    \"\"\"\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "\n",
        "    # Calculate mean of 1_month_total_return for winners and losers\n",
        "    winner_mean = data[data['IsWinner'] == 1]['1m_total_return'].mean()\n",
        "    loser_mean = data[data['IsLoser'] == 1]['1m_total_return'].mean()\n",
        "\n",
        "    # Append the results to the summary DataFrame\n",
        "    summary_df = summary_df.append({'date': start_date, 'winner_mean': winner_mean, 'loser_mean': loser_mean}, ignore_index=True)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############### calculates descriptive statistics for DataFrame tables (months)\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Path to your SQLite database\n",
        "db_path = '/content/drive/MyDrive/database/esp_hy_data_analysis.db'\n",
        "\n",
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Get the list of unique DataFrameIDs representing each month's data\n",
        "df_ids = pd.read_sql_query(\"SELECT DISTINCT DataFrameID FROM DataFrameEntries\", conn)\n",
        "\n",
        "# Initialize an empty DataFrame to hold descriptive statistics\n",
        "stats_summary = pd.DataFrame()\n",
        "\n",
        "for _, row in df_ids.iterrows():\n",
        "    df_id = row['DataFrameID']\n",
        "\n",
        "    # Query to retrieve 1_month_total_return for the current DataFrameID\n",
        "    query = f\"\"\"\n",
        "    SELECT 1m_total_return, IsWinner, IsLoser\n",
        "    FROM DataFrameEntries\n",
        "    WHERE DataFrameID = {df_id}\n",
        "    \"\"\"\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "\n",
        "    # Calculate descriptive statistics for the entire set, winners, and losers\n",
        "    desc_stats_entire = data['1m_total_return'].describe().rename('Entire Set')\n",
        "    desc_stats_winners = data[data['IsWinner'] == 1]['1m_total_return'].describe().rename('Winners')\n",
        "    desc_stats_losers = data[data['IsLoser'] == 1]['1m_total_return'].describe().rename('Losers')\n",
        "\n",
        "    # Compile descriptive statistics into a DataFrame\n",
        "    df_stats = pd.concat([desc_stats_entire, desc_stats_winners, desc_stats_losers], axis=1)\n",
        "    df_stats['DataFrameID'] = df_id  # Add DataFrameID for reference\n",
        "\n",
        "    # Append the stats of the current DataFrame to the summary DataFrame\n",
        "    stats_summary = pd.concat([stats_summary, df_stats], axis=0)\n",
        "\n",
        "    # Visualization (optional, as before)\n",
        "    # ...\n",
        "\n",
        "# Reset index of the summary DataFrame for better readability\n",
        "stats_summary.reset_index(inplace=True)\n",
        "stats_summary.rename(columns={'index': 'Statistic'}, inplace=True)\n",
        "\n",
        "# Optionally, save the stats_summary DataFrame to a CSV file\n",
        "stats_summary.to_csv('/content/drive/MyDrive/database/esp_hy_data_analysis_stats_summary.csv', index=False)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "# Display the aggregate statistics across all DataFrames\n",
        "aggregate_stats = stats_summary.groupby('Statistic').mean()  # Using mean here, but consider median or other aggregations depending on the data\n",
        "print(aggregate_stats)\n"
      ],
      "metadata": {
        "id": "C0v2bJuNLDkA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BS8jwuObLVu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlQrAtXnUy2wEne1bWOyPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}